# -*- coding: utf-8 -*-
"""Lstm_sgd_dropout(_4)_1aug.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ijDRN7Z_kexshZK5zK2C5kozu_hZ_2iC
"""

from google.colab import drive
drive.mount('/content/drive/')

from matplotlib import pyplot as plt
from tqdm import tqdm
import numpy as np
import tensorflow as tf
import os
import pickle
import json
import cv2
import re

data_dir = '/content/drive/My Drive/dataset/MMHS150K/'
model_dir = '/content/drive/My Drive/dataset/models/'

tweet_dict = json.load(open(data_dir + 'MMHS150K_GT.json', 'r'))

def hashtag(text):
    hashtag_body = text.group()[1:]
    if hashtag_body.isupper(): return "<hashtag> {} ".format(hashtag_body.lower())
    else: return ' '.join(["<hashtag>"] + [re.sub(r"([A-Z])",r" \1", hashtag_body, flags=re.MULTILINE | re.DOTALL)])

def allcaps(text): return text.group().lower() + ' <allcaps> ' 

def exclamation(text): return text.group().lower() + ' <exclamation> '

def question(text): return text.group().lower() + ' <question> '

def quotes(text): return text.group().lower() + ' <quotes> '

def clean_tweet_text(t):
    eyes = r'[8:=;]'
    nose = r"['`\-]?"
    
    t = re.sub(r'https?:\/\/\S+\b|www\.(\w+\.)+\S*', '<url>', t)
    t = re.sub(r'@\w+', '<user>', t)
    t = re.sub(r'{}{}[)dD]+|[)dD]+{}{}'.format(eyes, nose, nose, eyes), '<smile>', t)
    t = re.sub(r'{}{}p+".format(eyes, nose)', '<lolface>', t)
    t = re.sub(r'{}{}\(+|\)+{}{}'.format(eyes, nose, nose, eyes), '<sadface>', t)
    t = re.sub(r'{}{}[\/|l*]'.format(eyes, nose), '<neutralface>', t)
    t = re.sub(r'/', ' / ', t)
    t = re.sub(r'<3','<heart>', t)
    t = re.sub(r'[-+]?[.\d]*[\d]+[:,.\d]*', '<number>', t)
    t = re.sub(r'#\S+', hashtag, t)
    t = re.sub(r'([!?.]){2,}', r'\1 <repeat>', t)
    t = re.sub(r'\b(\S*?)(.)\2{2,}\b', r'\1\2 <elong>', t)
    t = re.sub(r'([A-Z]){2,}', allcaps, t)
    t = re.sub(r'{}'.format(r'[\".,-;&:]'), ' ', t)
    t = re.sub(r'{}'.format(r'[!]'),exclamation, t) 
    t = re.sub(r'{}'.format(r'[?]'),question, t)
    t = re.sub('[^a-zA-Z0-9]',' ',t)
    t = nltk.word_tokenize(t)
    t = [lemmatizer.lemmatize(word) for word in t if word not in set(stopwords.words('english'))]
    t = ' '.join(t)
    return t.lower()

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from tensorflow.keras.utils import to_categorical
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

lemmatizer = WordNetLemmatizer()
def get_data_list(path):
    data = []
    for id in open(data_dir + path, 'r').read().splitlines():

        text = tweet_dict[id]['tweet_text']
        text = clean_tweet_text(text)

        binary_labels = [1 if n > 0 else 0 for n in tweet_dict[id]['labels']]
        label = 1 if sum(binary_labels)/len(tweet_dict[id]['labels']) > 0.5 else 0

        data.append((text, label))

    return data

test_data = get_data_list('splits/val_ids.txt')
train_data = get_data_list('splits/test_ids.txt')
val_data = get_data_list('splits/val_ids.txt')
print('Train data len:', len(train_data))
print('Val data len:', len(val_data))
print('Test data len:', len(test_data))

# make the dataset
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

MAX_NUMBER_OF_WORDS = 20000
MAX_SEQ_LEN = 49

# training
texts, labels = zip(*train_data)
labels = to_categorical(labels)
print('Longest training sequence length:', max([len(t.split()) for t in texts]))

tokenizer = Tokenizer(num_words=MAX_NUMBER_OF_WORDS, filters='\t\n', lower=True)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

x_train = pad_sequences(sequences, maxlen=MAX_SEQ_LEN)

y_train = np.asarray(labels)
print('Shape of data tensor:', x_train.shape)
print('Shape of label tensor:', y_train.shape)

val_texts, val_labels = zip(*val_data)
val_labels = to_categorical(val_labels)
val_sequences = tokenizer.texts_to_sequences(val_texts) 
x_val = pad_sequences(val_sequences, maxlen=MAX_SEQ_LEN)
y_val = np.asarray(val_labels)

from tensorflow.keras.layers import Embedding
EMBEDDING_DIM = 100

embeddings_index = {}
for line in open(os.path.join('/content/drive/My Drive/dataset/glove', 'glove.twitter.27B.100d.txt')):
    values = line.split()
    word = values[0]
    embeddings_index[word] = np.asarray(values[1:], dtype='float32')

embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector

embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQ_LEN,
                            trainable=False)

from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import SGD
from tensorflow.keras import Sequential
from tensorflow.keras.utils import plot_model
model = Sequential()

model.add(embedding_layer)
model.add(LSTM(150))
model.add(Dropout(0.4))
model.add(Dense(512, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(SGD(lr= 1e-1,momentum=0.9),loss="categorical_crossentropy", metrics=['accuracy'])
print(model.summary())
# shuffle data
indices = (np.arange(x_train.shape[0]))
np.random.shuffle(indices)
x_train, y_train = x_train[indices], y_train[indices]

plot_model( model, to_file='model.png', show_shapes=True, show_dtype=False,
    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)

def step_decay(epoch):
  initial_rate = 1e-1
  factor = int(epoch / 10)
  lr = initial_rate / (10 ** factor)
  return lr

from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler
epochs_to_wait_for_improve=6
model_name = 'lstm_sgd_dropout(.4_new)_new'
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=epochs_to_wait_for_improve)
checkpoint_callback = ModelCheckpoint(model_dir + model_name+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')
lr_schedule = LearningRateScheduler(step_decay)
history = model.fit(x_train, y_train, epochs=30, batch_size=128, validation_data=(x_val, y_val),callbacks=[early_stopping_callback,checkpoint_callback, lr_schedule])

from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score

test_texts, test_labels = zip(*test_data)

test_sequences = tokenizer.texts_to_sequences(test_texts) # apply train tokenizer
x_test = pad_sequences(test_sequences, maxlen=MAX_SEQ_LEN)
y_test = np.asarray(test_labels)

# get AUROC
preds = model.predict(x_test)
pred_l = np.argmax(preds, axis=1)
print('Test AUROC:', roc_auc_score(y_test, pred_l))
print('Test acc:', accuracy_score(y_test, pred_l))

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
print('Confusion Matrix')
conf = confusion_matrix(y_test, pred_l)
print(conf)
print('----------------------------------------------------')
print(classification_report(y_test, pred_l))
print('----------------------------------------------------')

import pickle

model.save(model_dir + 'lstm_sgd_dropout(.4_new)_new.h5')
pickle.dump(tokenizer, open(model_dir + 'tokenizer_sgd_dropout(.4_new)_new.pkl', 'wb'))

pip install scikit-plot

import scikitplot as skplt
import matplotlib.pyplot as plt

skplt.metrics.plot_roc_curve(y_test, preds)
plt.show()