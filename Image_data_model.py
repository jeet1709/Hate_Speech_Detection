# -*- coding: utf-8 -*-
"""image-inceptionresnet-adam-2aug.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yMa5qqsaSEsRoaQZ7jV3vYrG8D9U_HDU
"""

from matplotlib import pyplot as plt
from tqdm import tqdm
import numpy as np
import tensorflow as tf
from PIL import Image
import os
import pickle
import json
import cv2
import re

data_dir = '/kaggle/input/multimodal-hate-speech/'
model_dir = '/kaggle/working/'

# load data and print sizes
tweet_dict = json.load(open(data_dir + 'MMHS150K_GT.json', 'r'))

from random import randint 
from tensorflow.keras.utils import to_categorical
class DataGenerator(tf.keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, splits_path, tweet_dict, batch_size, dim=(299, 299), n_channels=3, shuffle=True):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.n_channels = n_channels
        self.shuffle = shuffle
        
        # build labels list and id list
        self.id_list = open(splits_path, 'r').read().splitlines()
        self.labels = dict()
        for id in self.id_list:
            binary_labels = [1 if n > 0 else 0 for n in tweet_dict[id]['labels']]
            label = 1 if sum(binary_labels)/len(tweet_dict[id]['labels']) > 0.5 else 0
            self.labels[id] = label
            
        self.on_epoch_end()
        self.classes = [self.labels[self.id_list[i]] for i in self.indexes]

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.id_list) / self.batch_size)) + 1 # last batch is partial

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:index*self.batch_size + self.batch_size]
        
        
        # Find list of IDs
        id_list_temp = [self.id_list[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(id_list_temp)
        
        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.id_list))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, id_list_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X = np.empty((len(id_list_temp), *self.dim, self.n_channels))
        y = np.empty(len(id_list_temp), dtype=int)

        # Generate data
        for i, ID in enumerate(id_list_temp):
            # Store sample
            X[i,] = self.process_img(data_dir + 'img_resized/' + ID + '.jpg')

            # Store class
            y[i] = self.labels[ID]

        y = to_categorical(y)
        return X, y
    
    def process_img(self, path): # method for getting image
        img = Image.open(path)
        img.load()
        data = np.asarray(img, dtype='uint8')
        im = self.augment(data)
        
        if im.shape==(self.dim[0], self.dim[1]): im = np.stack((im,)*3, axis=-1) # handle grayscale
        
        return im
    
    def augment(self, im): # random crop and random mirror
        
        # random crop
        x_max, y_max = im.shape[0], im.shape[1]
        x_start, y_start = randint(0, x_max - self.dim[0]), randint(0, y_max - self.dim[1])
        im = im[x_start:x_start + self.dim[0], y_start:y_start + self.dim[1]]
        
        # random mirror
        if randint(0,1): im = np.flip(im, axis=1)
        
        return im

valid_gen = DataGenerator(splits_path=data_dir + 'splits/val_ids.txt',
                          tweet_dict=tweet_dict,
                          batch_size=32,
                          dim=(299, 299),
                          n_channels=3,
                          shuffle=True)

test_gen = DataGenerator(splits_path=data_dir + 'splits/val_ids.txt',
                          tweet_dict=tweet_dict,
                          batch_size=32,
                          dim=(299, 299),
                          n_channels=3,
                          shuffle=True)

train_gen = DataGenerator(splits_path=data_dir + 'splits/test_ids.txt',
                          tweet_dict=tweet_dict,
                          batch_size=32,
                          dim=(299, 299),
                          n_channels=3,
                          shuffle=True)

from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import plot_model
conv_base = tf.keras.applications.InceptionResNetV2(include_top=False, 
                                                        weights='imagenet', 
                                                        input_shape=(299, 299, 3))
# for layer in conv_base.layers[:-1]: layer.trainable = False # freeze pretrained layers

model = Sequential()
model.add(conv_base)
model.add(Flatten())
model.add(Dense(2048, activation='relu'))
model.add(Dense(1024, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(2, activation='softmax'))
print(model.summary())


model.compile(Adam(lr= 1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

plot_model( model, to_file='model.png', show_shapes=False, show_dtype=False,
    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)

def step_decay(epoch):
  initial_rate = 1e-4
  factor = int(epoch / 10)
  lr = initial_rate / (10 ** factor)
  return lr

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler
epochs_to_wait_for_improve=6
model_name = 'InceptionResnet1aug'
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=epochs_to_wait_for_improve)
checkpoint_callback = ModelCheckpoint(model_dir + model_name+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')
lr_schedule = LearningRateScheduler(step_decay)

history = model.fit_generator(train_gen,
                    validation_data=valid_gen,
                    shuffle=True,
                    epochs=30,callbacks=[early_stopping_callback,checkpoint_callback, lr_schedule])

#model.save(model_dir + model_name+'.h5')

from tensorflow.keras.models import load_model
model = load_model(model_dir + 'InceptionResnet1aug.h5')

from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score
import math

y_test = test_gen.classes

preds = model.predict_generator(test_gen,verbose=1)
pred_l = np.argmax(preds, axis=1)
print('Test AUROC:', roc_auc_score(y_test, pred_l))

print('Test Accuracy:', accuracy_score(y_test, pred_l))

print('Test F1:', f1_score(y_test, pred_l, zero_division=1))
print('Test Precision:', precision_score(y_test, pred_l, zero_division=1))
print('Test Recall:', recall_score(y_test, pred_l, zero_division=1))

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
print('Confusion Matrix')
conf = confusion_matrix(test_gen.classes, pred_l)
print(conf)
print('----------------------------------------------------')
print(classification_report(test_gen.classes, pred_l))
print('----------------------------------------------------')

pip install scikit-plot

import scikitplot as skplt
import matplotlib.pyplot as plt

skplt.metrics.plot_roc_curve(y_test, preds)
plt.show()