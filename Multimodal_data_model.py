# -*- coding: utf-8 -*-
"""Multimodal_InceptionResnet_sgd_2aug.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FB2sP_7J7vhvaoetyYXIP8XVJygMhfoP
"""

from google.colab import drive
drive.mount('/content/drive/')

from matplotlib import pyplot as plt
from tqdm import tqdm
import numpy as np
import tensorflow as tf
from PIL import Image
import os
import pickle
import json
import cv2
import re

data_dir = '/content/drive/My Drive/dataset/MMHS150K/'
model_dir = '/content/drive/My Drive/dataset/models/'

# load data and print sizes
tweet_dict = json.load(open(data_dir + 'MMHS150K_GT.json', 'r'))

from tensorflow.keras.models import load_model

lstm = load_model(model_dir + 'lstm_sgd_dropout(.4_new)_new.h5')
cnn = load_model(model_dir + 'InceptionResnet1aug.h5')

print(lstm.summary())
print(cnn.summary())

from tensorflow.keras.models import Sequential, Model

text_net = Sequential()
for layer in lstm.layers[:-1]: text_net.add(layer)
for i in range(len(text_net.layers)): text_net.layer[i]._handle_name = text_net.layer[i]._name + "_" + str(i)
print(text_net.summary())

img_net = Sequential()
for layer in cnn.layers[:-1]: img_net.add(layer)
for i in range(len(img_net.layers)): img_net.layer[i]._handle_name = img_net.layer[i]._name + "_i" + str(i)
print(img_net.summary())

import gc
gc.collect()

from tensorflow.keras.layers import concatenate
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import SGD

text_input = Input((text_net.layers[0].input_shape[-1],)) # get rid of None's in front
img_text_input = Input((text_net.layers[0].input_shape[-1],))
img_input = Input((img_net.layers[0].input_shape[1:])) # get rid of None in front

text_embed = text_net(text_input)
img_text_embed = text_net(img_text_input)
img_embed = img_net(img_input)

x = concatenate([text_embed, img_text_embed, img_embed])
x = Dense(512 + 512 + 512, activation='relu',name='concatenation')(x)
x = Dense(1024, activation='relu',name='prefinal')(x)
x = Dense(512, activation='relu',name='final')(x)
prediction = Dense(2, activation='softmax',name='output')(x)

fcm_model = Model(inputs=[text_input, img_text_input, img_input], outputs=prediction)
print(fcm_model.summary())

fcm_model.compile(SGD(lr= 1e-4,momentum=0.9),loss="categorical_crossentropy", metrics=['accuracy'])

for i in range(len(fcm_model.weights)):
    fcm_model.weights[i]._handle_name = fcm_model.weights[i].name + "_" + str(i)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

lemmatizer = WordNetLemmatizer()
def hashtag(text):
    hashtag_body = text.group()[1:]
    if hashtag_body.isupper(): return "<hashtag> {} ".format(hashtag_body.lower())
    else: return ' '.join(["<hashtag>"] + [re.sub(r"([A-Z])",r" \1", hashtag_body, flags=re.MULTILINE | re.DOTALL)])

def allcaps(text): return text.group().lower() + ' <allcaps> '    

def exclamation(text): return text.group().lower() + ' <exclamation> '

def question(text): return text.group().lower() + ' <question> '

def quotes(text): return text.group().lower() + ' <quotes> '

def clean_tweet_text(t):
    eyes = r'[8:=;]'
    nose = r"['`\-]?"
    
    t = re.sub(r'https?:\/\/\S+\b|www\.(\w+\.)+\S*', '<url>', t)
    t = re.sub(r'@\w+', '<user>', t)
    t = re.sub(r'{}{}[)dD]+|[)dD]+{}{}'.format(eyes, nose, nose, eyes), '<smile>', t)
    t = re.sub(r'{}{}p+".format(eyes, nose)', '<lolface>', t)
    t = re.sub(r'{}{}\(+|\)+{}{}'.format(eyes, nose, nose, eyes), '<sadface>', t)
    t = re.sub(r'{}{}[\/|l*]'.format(eyes, nose), '<neutralface>', t)
    t = re.sub(r'/', ' / ', t)
    t = re.sub(r'<3','<heart>', t)
    t = re.sub(r'[-+]?[.\d]*[\d]+[:,.\d]*', '<number>', t)
    t = re.sub(r'#\S+', hashtag, t)
    t = re.sub(r'([!?.]){2,}', r'\1 <repeat>', t)
    t = re.sub(r'\b(\S*?)(.)\2{2,}\b', r'\1\2 <elong>', t)
    t = re.sub(r'([A-Z]){2,}', allcaps, t)
    t = re.sub(r'{}'.format(r'[\".,-;&:]'), ' ', t)
    t = re.sub(r'{}'.format(r'[!]'),exclamation, t) 
    t = re.sub(r'{}'.format(r'[?]'),question, t)
    t = re.sub('[^a-zA-Z0-9]',' ',t)
    t = nltk.word_tokenize(t)
    t = [lemmatizer.lemmatize(word) for word in t if word not in set(stopwords.words('english'))]
    t = ' '.join(t)
    return t.lower()

from random import randint 
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences

class MMDataGenerator(tf.keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, splits_path, tweet_dict, tokenizer, pad_len, batch_size=32, dim=(299, 299), n_channels=3, 
                 shuffle=True):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.n_channels = n_channels
        self.shuffle = shuffle
        self.tweet_dict = tweet_dict
        self.tokenizer = tokenizer
        self.pad_len = pad_len
        
        # build labels list and id list
        self.id_list = open(splits_path, 'r').read().splitlines()
        self.labels = dict()
        for id in self.id_list:
            binary_labels = [1 if n > 0 else 0 for n in tweet_dict[id]['labels']]
            label = 1 if sum(binary_labels)/len(tweet_dict[id]['labels']) > 0.5 else 0
            self.labels[id] = label
        
        # create dictionary for embedded sequences (tuple of text and img_text)
        self.text_dict = self.process_text(self.id_list)
        
        self.on_epoch_end()
        self.classes = [self.labels[self.id_list[i]] for i in self.indexes]

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.id_list) / self.batch_size)) + 1 # last batch is partial

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:index*self.batch_size + self.batch_size]
        
        
        # Find list of IDs
        id_list_temp = [self.id_list[k] for k in indexes]

        # Generate data
        X_txt, X_img_txt, X_img, y = self.__data_generation(id_list_temp)
        
        return [X_txt, X_img_txt, X_img], y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.id_list))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, id_list_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X_img = np.empty((len(id_list_temp), *self.dim, self.n_channels))
        X_txt = np.empty((len(id_list_temp), self.pad_len))
        X_img_txt = np.empty((len(id_list_temp), self.pad_len))
        y = np.empty(len(id_list_temp), dtype=int)

        # Generate data
        for i, ID in enumerate(id_list_temp):
            X_img[i,] = self.process_img(data_dir + 'img_tweet/' + ID + '.jpg')
            X_txt[i,] = self.text_dict[ID][0]
            X_img_txt[i,] = self.text_dict[ID][1]

            # Store class
            y[i] = self.labels[ID]

        y = to_categorical(y)
        return X_txt, X_img_txt, X_img, y
    
    def process_img(self, path): # method for getting image
        img = Image.open(path)
        img.load()
        data = np.asarray(img, dtype='uint8')
        im = self.augment(data)
        
        if im.shape==(self.dim[0], self.dim[1]): im = np.stack((im,)*3, axis=-1) # handle grayscale
        
        return im
    
    def augment(self, im): # random crop and random mirror
        
        # random crop
        x_max, y_max = im.shape[0], im.shape[1]
        x_start, y_start = randint(0, x_max - self.dim[0]), randint(0, y_max - self.dim[1])
        im = im[x_start:x_start + self.dim[0], y_start:y_start + self.dim[1]]
        
        # random mirror
        if randint(0,1): im = np.flip(im, axis=1)
        
        return im
    
    def process_text(self, id_list):
        
        # matrix for texts
        texts = [clean_tweet_text(tweet_dict[ID]['tweet_text']) for ID in id_list]
        sequences = self.tokenizer.texts_to_sequences(texts)
        text_seqs = pad_sequences(sequences, maxlen=self.pad_len)
        
        # matrix for img_texts
        img_texts = []
        for ID in id_list:
            if os.path.exists(data_dir + 'img_txt/' + ID + '.json'):
                img_txt = json.load(open(data_dir + 'img_txt/' + ID + '.json', 'r'))['img_text']
                img_texts.append(img_txt)
            else: img_texts.append('')
        img_txt_sequences = self.tokenizer.texts_to_sequences(img_texts)
        img_text_seqs = pad_sequences(img_txt_sequences, maxlen=self.pad_len) 
        
        
        id_to_seq = dict() # map id to text sequence compatible with embedding layer
        for ID, txt, img_txt in zip(id_list, text_seqs, img_text_seqs):
            id_to_seq[ID] = (txt, img_txt)
        
        return id_to_seq

tokenizer = pickle.load(open(model_dir + 'tokenizer_sgd_dropout(.4_new)_new.pkl', 'rb'))
pad_len = 50

train_gen = MMDataGenerator(splits_path=data_dir + 'splits/test_ids.txt',
                          tweet_dict=tweet_dict,
                          tokenizer=tokenizer,
                          pad_len=pad_len,
                          batch_size=32,
                          dim=(299, 299),
                          n_channels=3,
                          shuffle=True)

val_gen = MMDataGenerator(splits_path=data_dir + 'splits/val_ids.txt',
                          tweet_dict=tweet_dict,
                          tokenizer=tokenizer,
                          pad_len=pad_len,
                          batch_size=32,
                          dim=(299, 299),
                          n_channels=3,
                          shuffle=True)

test_gen = MMDataGenerator(splits_path=data_dir + 'splits/val_ids.txt',
                          tweet_dict=tweet_dict,
                          tokenizer=tokenizer,
                          pad_len=pad_len,
                          batch_size=32,
                          dim=(299, 299),
                          n_channels=3,
                          shuffle=True)

def step_decay(epoch):
  initial_rate = 1e-4
  factor = int(epoch / 10)
  lr = initial_rate / (10 ** factor)
  return lr

from tensorflow.keras.callbacks import ModelCheckpoint

from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler
epochs_to_wait_for_improve=6
model_name = 'mulincsgdf'
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=epochs_to_wait_for_improve)
checkpoint_callback = ModelCheckpoint(model_dir + model_name+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')
lr_schedule = LearningRateScheduler(step_decay)

history = fcm_model.fit_generator(train_gen,validation_data=val_gen,shuffle=True,epochs=30,callbacks=[early_stopping_callback,checkpoint_callback, lr_schedule])

model.save(model_dir + 'Bestmultimodal1_sgd_finalmay.h5')

fcm_model = load_model(model_dir + 'Bestmultimodal_sgd_new.h5')

from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score
import math

y_test = test_gen.classes

preds = fcm_model.predict_generator(test_gen,verbose=1)
pred_l = np.argmax(preds, axis=1)
print('Test AUROC:', roc_auc_score(y_test, pred_l))
print('Test Accuracy:', accuracy_score(y_test, pred_l))

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
print('Confusion Matrix')
conf = confusion_matrix(test_gen.classes, pred_l)
print(conf)
print('----------------------------------------------------')
print(classification_report(test_gen.classes, pred_l))
print('----------------------------------------------------')

pip install scikit-plot

import scikitplot as skplt
import matplotlib.pyplot as plt

skplt.metrics.plot_roc_curve(y_test, preds)
plt.show()